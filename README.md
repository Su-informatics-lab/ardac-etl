# ardac-etl
ARDaC ETL (Extract Transform Load) workflows for DCC data release version 2.0.0.  Currently, the workflows transform observational and clinical data stored in CSV files, to ARDaC node files in TSV format.

# Installation
The python workflow is implemented and tested with python 3.13.2.  Furthermore, the NextFlow workflow scripts
utilize Anaconda Python virtual environments.  Anaconda is chosen over the Python built-in virtual environment
manager because NextFlow has a strong preference for using Anaconda virtual environments.

If Anaconda is not already installed on your system, the small-scale Anaconda environment manager called
_miniconda_ can be installed easily using the following [installation instructions](https://www.anaconda.com/docs/getting-started/miniconda/install#macos-linux-installation)

The Anaconda base installation should be availble in your shell's search path after installation.  Open a new
terminal window so that you are using the updated environment.

To show your current search path:
```
echo $PATH
```

The path should contain the installtion location of Anaconda.  If not, you should seek help installing Anaconda
from from your system administrator.

The next task is to create the virtual environment using Anaconda.

## Create the virtual environment
- If `base` environment is not activated, do `conda activate base`. If you are in another virtual environment, then deactivate it first.
- `cd /path/to/ardac-etl/python`
- `conda env create -p ../venv -f conda.yml` creates a virtual environment at `/path/to/project/ardac-etl/venv` with packages and python installed accoridng to `conda.yml` 

The created Python virtual environment at `/path/to/project/ardac-etl/venv` should be used for development of the
ardac-etl project and used with the NextFlow workflow.  If you are using VSCode as your IDE, you should set the
following configuration fields in the VSCode settings accordingly:
```
Python: Venv Path = ./venv
Python: Conda Path = /path/to/your/conda/bin/conda
```

You may also need to set `Python: Locator = js` if the IDE cycles indefinitely on `Reactivating Terminals`.

## Installing NextFlow
Follow the instructions [here](https://www.nextflow.io/docs/latest/install.html) to install NextFlow

# The ARDaC mapper workflow
The mapper workflow is responsible for tranforming the observational and clinical data stored in CSV format to ARDaC node files which conform to the ARDaC common data model (CDM).

## Configuring the ARDaC mapper workflow
The ARDaC mapper workflow is implemented using python scripts to generate the individual node files, and a NextFlow script which is used to manage the execution of the python scripts and manage any dependencies needed by each python process.  The workflow can be configured to read the input and write the output from and to a file system approved for sensitive clinical data.  The workflow can also be configured to store any intermediate data generated by the workflow on the same approved file system.  Logs for each process in the workflow are also created on the approved file system, to reduce chance of spilling sensitive information to an unapproved file system.

The workflow configuration is set by the `nextflow.config` file.  A template is provided by `nextflow.config.template`.  In this configuration file you may specify the location of the input observational and clinical data and the output directory which contains the ARDaC CDM nodes and process log files.

The input directory, given by the parameter `params.input_directory` which must specify the full path, must contain three subdirectories: one given by the parameter `params.obs_input_directory` containing the observational data, one give by `params.rct_input_directory` containing the clinical data, and one given by the parameter `params.node_templates_directory` containing the ARDaC node template TSV file.  The output directory, given by the parameter `params.output_directory` which must specify the full path, must contain a node subdirectory given by `params`.ardac_nodes_directory` where the ARDaC node files generated by the workflow will be saved.  Detailed comments are provided for each parameter in the configuration file.

## Running the ARDaC mapper workflow
The NextFlow workflow to generate the ARDaC nodes from the observational and clinical data sets is performed by the `run_observational_workflow.bash` and `run_clinical_workflow.bash` scripts in the `nextflow` subdirectory.  These scripts can be run directly in that same subdirectory.  A hidden log file `.nextflow.log` will be generated describing the run and any problems that may have occurred.

### Execution modes
The workflow can be executed in HPC batch processing environments under the control of a job scheduler like SLURM.  Currently, the workflow can run only on a single node.  The jobs may be interactive jobs used for debugging and development, or they may be queued to run the workflow in production.  If you have access to an HPC resource that has access to the data sets that need translation, you can use that system for development and production execution of the workflow.

The Git project should be cloned and the appropriate branch checked out for testing before execution.  The NextFlow configuration file will need to be updated so that the input data can be located on the system, and so that the ARDaC node files can be written to an appropriate destination.

#### Interactive execution
To execute the workflow as an interactive job:

1. Change to the `nextflow` directory
2. Create an interactive job and log into the interactive node: 
   `srun -A xxxxxx -p debug -N 1 --time=01:00:00 --pty bash`
   where `--time` indicates how long the node will be reserved for use,
   and where `xxxxxx` is an account the hour of run time will be charged to
3. Execute one of the workflow scripts `run_clinical_workflow.bash` or `run_observational_workflow.bash`

#### Batch job execution
The workflow script should be executed through a SLURM batch submission script that can specify how the job is to be executed.  This script is the same as a shell script, but with directives for how SLURM should select resources to run the job.  Below is an example batch submission script:

```bash
#!/bin/bash
#SBATCH -J <job_name>
#SBATCH -p <queue_name>
#SBATCH -o %x_%j.txt
#SBATCH -e %x_%j.err
#SBATCH --mail-type=ALL
#SBATCH --mail-user=jmccombs@iu.edu
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --time=00:15:00
#SBATCH --mem=8G
#SBATCH -A <project_account>

conda activate ../venv
./run_clinical_workflow.bash
```

The directives to SLURM are given as bash shell comments.  The `-J` specifies the name assigned to the job.  The `-p` specifies the name of the queue the job will be submitted to.  The `-o` and `-e` directives are files the standard output and standard error produced by the job will be redirected to.  The mail directives specify which statuses will trigger an email notification and the e-mail account the notifications will be sent to.  The `--nodes` directive specifies that a single node will be used for executing the job.  The `--ntasks-per-node` directive specifies how many instances of the workflow script will be executed on each node.  The `--time` directive sets the maximum amount of time the job can run before it is terminated by the scheduler.  The `--mem` directive specifies the maximum amount of RAM can be used on each node for the job.  The `-A` directive specifies the project account the total node-hours will be charged to.

To submit the job, execute the command:
```bash
sbatch <submission_script>
```

## Workflow versioning
The python scripts perform the mapping from the observational and clinical Data Coordinating Center (DCC) format to the ARDaC CDM node format.  The DCC format currently supported by the mappers is set in the `_constants.py` file in the `python/ardac` script directory under the parameter `__dcc_data_release__`.  The parameter `__mapping_version__` sets the version of the mapping software which implements the mapping for the current DCC release.  If the mapping for a particular DCC data release is to be updated, then the mapping version should be increased.  If support for a new DCC data release is to be implemented, then the DCC release version should be increased and the mapping version reset to `1.0.0`.

The NextFlow workflow script is built for a particular DCC data release.  The DCC data release version expected by the NextFlow workflow is set in the `nextflow.config` file under the `params.dcc_release` configuration parameter.

# Git branch management
The `main` branch is expected to contain the latest tested mapping implementation for production use. Git tags should be used to indicate which main branch version corresponds to a particular DCC release and mapping version.  Development for a new mapping version should occur in the `develop` branch.  Any development of new features should occur in subbranches of the `develop` branch and merged into the `develop` branch as they are completed.  The `develop` branch should also be used for integration of multiple new features through merging of the feature branches into `develop`.  Once the `develop` branch is fully tested, the version parameters can be updated accordingly and the branch merged into `main` and tagged for prodcution release.  This process is detailed in the `development.md` file in the `docs` subdirectory.



